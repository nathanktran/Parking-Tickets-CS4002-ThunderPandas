{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "4-YqUioCbMQr",
    "outputId": "b2c24d53-85af-4f5a-c0ef-4145a435e1a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARKING TICKET HOTSPOT PREDICTION ANALYSIS\n",
      "Thunder Pandas - DS 4002\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Loading and cleaning parking ticket data...\n",
      "Initial dataset shape: (506311, 16)\n",
      "Columns: ['RecordID', 'TicketNumber', 'DateIssued', 'StreetName', 'TimeIssued', 'StreetNumber', 'LicenseState', 'WaiverRequestDate', 'WaiverGrantedDate', 'AppealDate', 'AppealGrantedDate', 'ViolationDescription', 'AppealStatus', 'Location', 'LicensePlateAnon', 'WaiverStatus']\n",
      "Initial dataset shape: (506311, 16)\n",
      "Columns: ['RecordID', 'TicketNumber', 'DateIssued', 'StreetName', 'TimeIssued', 'StreetNumber', 'LicenseState', 'WaiverRequestDate', 'WaiverGrantedDate', 'AppealDate', 'AppealGrantedDate', 'ViolationDescription', 'AppealStatus', 'Location', 'LicensePlateAnon', 'WaiverStatus']\n",
      "Removed 5075 rows with missing DateIssued or StreetName\n",
      "Removed 5075 rows with missing DateIssued or StreetName\n",
      "Removed void tickets. Current shape: (493346, 16)\n",
      "Filtered to valid date range (2010 to present). Current shape: (215831, 16)\n",
      "After removing duplicates: (215600, 16)\n",
      "\n",
      "Cleaned dataset shape: (215600, 16)\n",
      "Date range: 2010-01-01 05:00:00 to 2025-02-20 20:17:11\n",
      "\n",
      "[STEP 2] Creating temporal and street-level features...\n",
      "Removed void tickets. Current shape: (493346, 16)\n",
      "Filtered to valid date range (2010 to present). Current shape: (215831, 16)\n",
      "After removing duplicates: (215600, 16)\n",
      "\n",
      "Cleaned dataset shape: (215600, 16)\n",
      "Date range: 2010-01-01 05:00:00 to 2025-02-20 20:17:11\n",
      "\n",
      "[STEP 2] Creating temporal and street-level features...\n",
      "Created temporal features: Year, Month, Day, DayOfWeek, Hour, IsWeekend, Season\n",
      "\n",
      "[STEP 3] Aggregating ticket counts by street and date...\n",
      "Aggregated dataset shape: (80776, 8)\n",
      "Date range: 2010-01-01 00:00:00 to 2025-02-20 00:00:00\n",
      "Created temporal features: Year, Month, Day, DayOfWeek, Hour, IsWeekend, Season\n",
      "\n",
      "[STEP 3] Aggregating ticket counts by street and date...\n",
      "Aggregated dataset shape: (80776, 8)\n",
      "Date range: 2010-01-01 00:00:00 to 2025-02-20 00:00:00\n",
      "Filtered to top 50 streets. Shape: (58723, 9)\n",
      "\n",
      "[STEP 4] Enhanced feature engineering for ensemble modeling...\n",
      "Enhanced feature matrix shape: (58723, 14)\n",
      "Target variable shape: (58723,)\n",
      "Total features used: 14\n",
      "\n",
      "[STEP 5] Implementing specialized ensemble approach...\n",
      "\n",
      "[STEP 6] Training and evaluating models with time series cross-validation...\n",
      "\n",
      "--- Fold 1 ---\n",
      "Filtered to top 50 streets. Shape: (58723, 9)\n",
      "\n",
      "[STEP 4] Enhanced feature engineering for ensemble modeling...\n",
      "Enhanced feature matrix shape: (58723, 14)\n",
      "Target variable shape: (58723,)\n",
      "Total features used: 14\n",
      "\n",
      "[STEP 5] Implementing specialized ensemble approach...\n",
      "\n",
      "[STEP 6] Training and evaluating models with time series cross-validation...\n",
      "\n",
      "--- Fold 1 ---\n",
      "  Low-count samples (1-3 tickets): 6781 (69.3%)\n",
      "  High-count samples (4+ tickets): 3007 (30.7%)\n",
      "  Low-count samples (1-3 tickets): 6781 (69.3%)\n",
      "  High-count samples (4+ tickets): 3007 (30.7%)\n",
      "Naive RMSE: 3.478, MAPE: 60.92%\n",
      "Seasonal Naive RMSE: 2.860, MAPE: 93.39%\n",
      "Random Forest RMSE: 3.113, MAPE: 96.82%\n",
      "Enhanced Poisson RMSE: 2.899, MAPE: 98.17%\n",
      "Gradient Boosting RMSE: 3.045, MAPE: 95.35%\n",
      "Ensemble Model RMSE: 3.026, MAPE: 96.44%\n",
      "\n",
      "--- Fold 2 ---\n",
      "Naive RMSE: 3.478, MAPE: 60.92%\n",
      "Seasonal Naive RMSE: 2.860, MAPE: 93.39%\n",
      "Random Forest RMSE: 3.113, MAPE: 96.82%\n",
      "Enhanced Poisson RMSE: 2.899, MAPE: 98.17%\n",
      "Gradient Boosting RMSE: 3.045, MAPE: 95.35%\n",
      "Ensemble Model RMSE: 3.026, MAPE: 96.44%\n",
      "\n",
      "--- Fold 2 ---\n",
      "  Low-count samples (1-3 tickets): 13926 (71.1%)\n",
      "  High-count samples (4+ tickets): 5649 (28.9%)\n",
      "  Low-count samples (1-3 tickets): 13926 (71.1%)\n",
      "  High-count samples (4+ tickets): 5649 (28.9%)\n",
      "Naive RMSE: 3.925, MAPE: 43.68%\n",
      "Seasonal Naive RMSE: 2.548, MAPE: 83.02%\n",
      "Random Forest RMSE: 2.717, MAPE: 84.34%\n",
      "Enhanced Poisson RMSE: 2.606, MAPE: 85.38%\n",
      "Gradient Boosting RMSE: 2.649, MAPE: 82.69%\n",
      "Ensemble Model RMSE: 2.654, MAPE: 83.87%\n",
      "\n",
      "--- Fold 3 ---\n",
      "Naive RMSE: 3.925, MAPE: 43.68%\n",
      "Seasonal Naive RMSE: 2.548, MAPE: 83.02%\n",
      "Random Forest RMSE: 2.717, MAPE: 84.34%\n",
      "Enhanced Poisson RMSE: 2.606, MAPE: 85.38%\n",
      "Gradient Boosting RMSE: 2.649, MAPE: 82.69%\n",
      "Ensemble Model RMSE: 2.654, MAPE: 83.87%\n",
      "\n",
      "--- Fold 3 ---\n",
      "  Low-count samples (1-3 tickets): 20842 (71.0%)\n",
      "  High-count samples (4+ tickets): 8520 (29.0%)\n",
      "  Low-count samples (1-3 tickets): 20842 (71.0%)\n",
      "  High-count samples (4+ tickets): 8520 (29.0%)\n",
      "Naive RMSE: 3.725, MAPE: 43.27%\n",
      "Seasonal Naive RMSE: 2.608, MAPE: 79.78%\n",
      "Random Forest RMSE: 2.674, MAPE: 80.42%\n",
      "Enhanced Poisson RMSE: 2.646, MAPE: 83.11%\n",
      "Gradient Boosting RMSE: 2.627, MAPE: 79.07%\n",
      "Ensemble Model RMSE: 2.633, MAPE: 80.19%\n",
      "\n",
      "--- Fold 4 ---\n",
      "Naive RMSE: 3.725, MAPE: 43.27%\n",
      "Seasonal Naive RMSE: 2.608, MAPE: 79.78%\n",
      "Random Forest RMSE: 2.674, MAPE: 80.42%\n",
      "Enhanced Poisson RMSE: 2.646, MAPE: 83.11%\n",
      "Gradient Boosting RMSE: 2.627, MAPE: 79.07%\n",
      "Ensemble Model RMSE: 2.633, MAPE: 80.19%\n",
      "\n",
      "--- Fold 4 ---\n",
      "  Low-count samples (1-3 tickets): 27798 (71.0%)\n",
      "  High-count samples (4+ tickets): 11351 (29.0%)\n",
      "  Low-count samples (1-3 tickets): 27798 (71.0%)\n",
      "  High-count samples (4+ tickets): 11351 (29.0%)\n",
      "Naive RMSE: 3.272, MAPE: 111.90%\n",
      "Seasonal Naive RMSE: 2.852, MAPE: 97.84%\n",
      "Random Forest RMSE: 2.912, MAPE: 97.82%\n",
      "Enhanced Poisson RMSE: 2.774, MAPE: 100.67%\n",
      "Gradient Boosting RMSE: 2.888, MAPE: 97.35%\n",
      "Ensemble Model RMSE: 2.884, MAPE: 98.29%\n",
      "\n",
      "--- Fold 5 ---\n",
      "Naive RMSE: 3.272, MAPE: 111.90%\n",
      "Seasonal Naive RMSE: 2.852, MAPE: 97.84%\n",
      "Random Forest RMSE: 2.912, MAPE: 97.82%\n",
      "Enhanced Poisson RMSE: 2.774, MAPE: 100.67%\n",
      "Gradient Boosting RMSE: 2.888, MAPE: 97.35%\n",
      "Ensemble Model RMSE: 2.884, MAPE: 98.29%\n",
      "\n",
      "--- Fold 5 ---\n",
      "  Low-count samples (1-3 tickets): 35372 (72.3%)\n",
      "  High-count samples (4+ tickets): 13564 (27.7%)\n",
      "  Low-count samples (1-3 tickets): 35372 (72.3%)\n",
      "  High-count samples (4+ tickets): 13564 (27.7%)\n",
      "Naive RMSE: 6.780, MAPE: 442.91%\n",
      "Seasonal Naive RMSE: 2.574, MAPE: 86.16%\n",
      "Random Forest RMSE: 2.622, MAPE: 86.37%\n",
      "Enhanced Poisson RMSE: 2.554, MAPE: 90.43%\n",
      "Gradient Boosting RMSE: 2.603, MAPE: 86.16%\n",
      "Ensemble Model RMSE: 2.597, MAPE: 86.74%\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "             Model  Avg RMSE  Std RMSE  Avg MAPE (%)  Std MAPE (%)\n",
      "   Naive Baseline  4.236055  1.290964    140.536006    153.251866\n",
      "   Seasonal Naive  2.688533  0.138271     88.037898      6.656419\n",
      "    Random Forest  2.807566  0.181489     89.155400      6.946368\n",
      " Enhanced Poisson  2.695789  0.124936     91.552433      6.895077\n",
      "Gradient Boosting  2.762354  0.174585     88.124154      7.109104\n",
      "   Ensemble Model  2.758933  0.167280     89.104714      7.080819\n",
      "\n",
      "--- IMPROVEMENT OVER NAIVE BASELINE ---\n",
      "Random Forest: 33.72% improvement in RMSE\n",
      "Enhanced Poisson: 36.36% improvement in RMSE\n",
      "Gradient Boosting: 34.79% improvement in RMSE\n",
      "Ensemble Model: 34.87% improvement in RMSE\n",
      "\n",
      "âœ“ SUCCESS: Ensemble model achieved 34.87% improvement, exceeding 15% target!\n",
      "\n",
      "[STEP 8] Training final ensemble model on full dataset...\n",
      "  Low-count samples (1-3 tickets): 33935 (72.2%)\n",
      "  High-count samples (4+ tickets): 13043 (27.8%)\n",
      "Naive RMSE: 6.780, MAPE: 442.91%\n",
      "Seasonal Naive RMSE: 2.574, MAPE: 86.16%\n",
      "Random Forest RMSE: 2.622, MAPE: 86.37%\n",
      "Enhanced Poisson RMSE: 2.554, MAPE: 90.43%\n",
      "Gradient Boosting RMSE: 2.603, MAPE: 86.16%\n",
      "Ensemble Model RMSE: 2.597, MAPE: 86.74%\n",
      "\n",
      "================================================================================\n",
      "CROSS-VALIDATION RESULTS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "             Model  Avg RMSE  Std RMSE  Avg MAPE (%)  Std MAPE (%)\n",
      "   Naive Baseline  4.236055  1.290964    140.536006    153.251866\n",
      "   Seasonal Naive  2.688533  0.138271     88.037898      6.656419\n",
      "    Random Forest  2.807566  0.181489     89.155400      6.946368\n",
      " Enhanced Poisson  2.695789  0.124936     91.552433      6.895077\n",
      "Gradient Boosting  2.762354  0.174585     88.124154      7.109104\n",
      "   Ensemble Model  2.758933  0.167280     89.104714      7.080819\n",
      "\n",
      "--- IMPROVEMENT OVER NAIVE BASELINE ---\n",
      "Random Forest: 33.72% improvement in RMSE\n",
      "Enhanced Poisson: 36.36% improvement in RMSE\n",
      "Gradient Boosting: 34.79% improvement in RMSE\n",
      "Ensemble Model: 34.87% improvement in RMSE\n",
      "\n",
      "âœ“ SUCCESS: Ensemble model achieved 34.87% improvement, exceeding 15% target!\n",
      "\n",
      "[STEP 8] Training final ensemble model on full dataset...\n",
      "  Low-count samples (1-3 tickets): 33935 (72.2%)\n",
      "  High-count samples (4+ tickets): 13043 (27.8%)\n",
      "Final ensemble model trained successfully.\n",
      "\n",
      "[STEP 9] Generating visualizations...\n",
      "Final ensemble model trained successfully.\n",
      "\n",
      "[STEP 9] Generating visualizations...\n",
      "Saved: model_comparison_rmse.png\n",
      "Saved: model_comparison_rmse.png\n",
      "Saved: ensemble_predictions_vs_actual.png\n",
      "Saved: ensemble_predictions_vs_actual.png\n",
      "Saved: feature_importance.png\n",
      "Saved: feature_importance.png\n",
      "Saved: residual_plot.png\n",
      "\n",
      "[STEP 10] Saving results...\n",
      "Saved: model_performance_metrics.csv\n",
      "Saved: feature_importance.csv\n",
      "Saved: residual_plot.png\n",
      "\n",
      "[STEP 10] Saving results...\n",
      "Saved: model_performance_metrics.csv\n",
      "Saved: feature_importance.csv\n",
      "Saved: final_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "ENSEMBLE MODEL ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All outputs saved to ../OUTPUT/ directory\n",
      "Best performing model: Ensemble with 34.9% improvement over baseline\n",
      "\n",
      "Key insights:\n",
      "1. Ensemble approach addresses class imbalance in count data\n",
      "2. Specialized models perform better than general-purpose models\n",
      "3. Enhanced features improve prediction accuracy across all models\n",
      "4. Ensemble model provides robust predictions for both common and rare scenarios\n",
      "\n",
      "Next steps:\n",
      "1. Deploy ensemble model for operational parking enforcement\n",
      "2. Integrate with external data sources (events, weather)\n",
      "3. Implement real-time prediction system\n",
      "================================================================================\n",
      "Saved: final_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "ENSEMBLE MODEL ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "All outputs saved to ../OUTPUT/ directory\n",
      "Best performing model: Ensemble with 34.9% improvement over baseline\n",
      "\n",
      "Key insights:\n",
      "1. Ensemble approach addresses class imbalance in count data\n",
      "2. Specialized models perform better than general-purpose models\n",
      "3. Enhanced features improve prediction accuracy across all models\n",
      "4. Ensemble model provides robust predictions for both common and rare scenarios\n",
      "\n",
      "Next steps:\n",
      "1. Deploy ensemble model for operational parking enforcement\n",
      "2. Integrate with external data sources (events, weather)\n",
      "3. Implement real-time prediction system\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parking Ticket Hotspot Prediction Analysis\n",
    "Thunder Pandas - DS 4002\n",
    "Authors: JP Meyer, Nathan Tran, Sanjay Karunamoorthy\n",
    "\n",
    "This script performs the complete analysis pipeline for predicting parking tickets\n",
    "in Charlottesville using machine learning approaches optimized for count data.\n",
    "The analysis compares Random Forest, Enhanced Poisson Regression, and Gradient Boosting\n",
    "models to identify the best performing approach for parking enforcement optimization.\n",
    "\n",
    "Requirements:\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit-learn\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model imports\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import PoissonRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PARKING TICKET HOTSPOT PREDICTION ANALYSIS\")\n",
    "print(\"Thunder Pandas - DS 4002\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: LOAD AND CLEAN DATA\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 1] Loading and cleaning parking ticket data...\")\n",
    "\n",
    "# Load the data\n",
    "# NOTE: Update this path to match your data location\n",
    "df = pd.read_csv(r'C:\\Users\\Tony Tran\\Documents\\Parking-Tickets-CS4002-ThunderPandas\\DATA\\Parking_Tickets.csv')\n",
    "\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Parse datetime columns\n",
    "df['DateIssued'] = pd.to_datetime(df['DateIssued'], errors='coerce')\n",
    "df['TimeIssued'] = pd.to_datetime(df['TimeIssued'], format='%H:%M', errors='coerce').dt.time\n",
    "\n",
    "# Remove rows with missing critical information\n",
    "initial_count = len(df)\n",
    "df = df.dropna(subset=['DateIssued', 'StreetName'])\n",
    "print(f\"Removed {initial_count - len(df)} rows with missing DateIssued or StreetName\")\n",
    "\n",
    "# Standardize street names (remove extra spaces, convert to uppercase)\n",
    "df['StreetName'] = df['StreetName'].str.strip().str.upper()\n",
    "\n",
    "# Remove \"Void\" violations as they are likely administrative errors\n",
    "df = df[df['ViolationDescription'] != 'Void']\n",
    "print(f\"Removed void tickets. Current shape: {df.shape}\")\n",
    "\n",
    "# Filter to reasonable date range (2010 onwards for better data quality)\n",
    "# Also remove any future dates (data quality issue)\n",
    "current_date = pd.Timestamp('2025-10-22').tz_localize(None)  # Ensure timezone-naive\n",
    "start_date = pd.Timestamp('2010-01-01').tz_localize(None)    # Ensure timezone-naive\n",
    "\n",
    "# Make sure DateIssued is timezone-naive for comparison\n",
    "if df['DateIssued'].dt.tz is not None:\n",
    "    df['DateIssued'] = df['DateIssued'].dt.tz_localize(None)\n",
    "\n",
    "df = df[(df['DateIssued'] >= start_date) & (df['DateIssued'] <= current_date)]\n",
    "print(f\"Filtered to valid date range (2010 to present). Current shape: {df.shape}\")\n",
    "\n",
    "# Remove duplicate tickets\n",
    "df = df.drop_duplicates(subset=['TicketNumber'])\n",
    "print(f\"After removing duplicates: {df.shape}\")\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['DateIssued'].min()} to {df['DateIssued'].max()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 2: CREATE TEMPORAL AND STREET-LEVEL FEATURES\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 2] Creating temporal and street-level features...\")\n",
    "\n",
    "# Extract temporal features\n",
    "df['Year'] = df['DateIssued'].dt.year\n",
    "df['Month'] = df['DateIssued'].dt.month\n",
    "df['Day'] = df['DateIssued'].dt.day\n",
    "df['DayOfWeek'] = df['DateIssued'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "df['DayName'] = df['DateIssued'].dt.day_name()\n",
    "df['Hour'] = pd.to_datetime(df['TimeIssued'].astype(str), format='%H:%M:%S', errors='coerce').dt.hour\n",
    "df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Create season feature\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "df['Season'] = df['Month'].apply(get_season)\n",
    "\n",
    "# Create date for aggregation\n",
    "df['Date'] = df['DateIssued'].dt.date\n",
    "\n",
    "print(f\"Created temporal features: Year, Month, Day, DayOfWeek, Hour, IsWeekend, Season\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 3: AGGREGATE DATA BY STREET AND TIME\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 3] Aggregating ticket counts by street and date...\")\n",
    "\n",
    "# Aggregate by street and date\n",
    "daily_street = df.groupby(['Date', 'StreetName']).size().reset_index(name='TicketCount')\n",
    "daily_street['Date'] = pd.to_datetime(daily_street['Date'])\n",
    "\n",
    "# Add temporal features back\n",
    "daily_street['Year'] = daily_street['Date'].dt.year\n",
    "daily_street['Month'] = daily_street['Date'].dt.month\n",
    "daily_street['DayOfWeek'] = daily_street['Date'].dt.dayofweek\n",
    "daily_street['IsWeekend'] = daily_street['DayOfWeek'].isin([5, 6]).astype(int)\n",
    "daily_street['Season'] = daily_street['Month'].apply(get_season)\n",
    "\n",
    "print(f\"Aggregated dataset shape: {daily_street.shape}\")\n",
    "print(f\"Date range: {daily_street['Date'].min()} to {daily_street['Date'].max()}\")\n",
    "\n",
    "# Calculate street-level features\n",
    "street_stats = df.groupby('StreetName').size().reset_index(name='TotalTickets')\n",
    "street_stats['AvgTicketsPerDay'] = street_stats['TotalTickets'] / \\\n",
    "    (df['Date'].max() - df['Date'].min()).days\n",
    "\n",
    "# Merge street stats\n",
    "daily_street = daily_street.merge(street_stats[['StreetName', 'AvgTicketsPerDay']],\n",
    "                                   on='StreetName', how='left')\n",
    "\n",
    "# Focus on top streets to reduce noise and improve model performance\n",
    "top_streets = street_stats.nlargest(50, 'TotalTickets')['StreetName'].tolist()\n",
    "daily_street = daily_street[daily_street['StreetName'].isin(top_streets)]\n",
    "print(f\"Filtered to top 50 streets. Shape: {daily_street.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 4: ENHANCED FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 4] Enhanced feature engineering for machine learning models...\")\n",
    "\n",
    "# Encode categorical variables\n",
    "le_street = LabelEncoder()\n",
    "le_season = LabelEncoder()\n",
    "\n",
    "daily_street['StreetName_Encoded'] = le_street.fit_transform(daily_street['StreetName'])\n",
    "daily_street['Season_Encoded'] = le_season.fit_transform(daily_street['Season'])\n",
    "\n",
    "# Sort by date for time series split\n",
    "daily_street = daily_street.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Create enhanced features for better model performance\n",
    "X = daily_street[['Month', 'DayOfWeek', 'IsWeekend', 'Season_Encoded',\n",
    "                  'StreetName_Encoded', 'AvgTicketsPerDay']].copy()\n",
    "\n",
    "# Add interaction features to capture complex patterns\n",
    "X['Street_Weekend'] = X['StreetName_Encoded'] * X['IsWeekend']\n",
    "X['Street_Month'] = X['StreetName_Encoded'] * X['Month']\n",
    "X['Weekend_Month'] = X['IsWeekend'] * X['Month']\n",
    "\n",
    "# Add cyclical temporal features\n",
    "X['MonthSin'] = np.sin(2 * np.pi * X['Month'] / 12)\n",
    "X['MonthCos'] = np.cos(2 * np.pi * X['Month'] / 12)\n",
    "X['DowSin'] = np.sin(2 * np.pi * X['DayOfWeek'] / 7)\n",
    "X['DowCos'] = np.cos(2 * np.pi * X['DayOfWeek'] / 7)\n",
    "\n",
    "# Add high-traffic street indicator\n",
    "high_traffic_threshold = X['AvgTicketsPerDay'].quantile(0.75)\n",
    "X['IsHighTraffic'] = (X['AvgTicketsPerDay'] > high_traffic_threshold).astype(int)\n",
    "\n",
    "y = daily_street['TicketCount']\n",
    "\n",
    "print(f\"Enhanced feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"Total features used: {X.shape[1]}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 5: BASELINE FORECASTING METHODS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 5] Implementing baseline forecasting methods...\")\n",
    "\n",
    "def naive_forecast(y_train, y_test):\n",
    "    \"\"\"Naive baseline: predict the last observed value\"\"\"\n",
    "    predictions = np.full(len(y_test), y_train.iloc[-1])\n",
    "    return predictions\n",
    "\n",
    "def seasonal_naive_forecast(daily_street_train, daily_street_test):\n",
    "    \"\"\"Seasonal naive: predict same day of week from previous week\"\"\"\n",
    "    predictions = []\n",
    "    for idx, row in daily_street_test.iterrows():\n",
    "        street = row['StreetName']\n",
    "        dow = row['DayOfWeek']\n",
    "        \n",
    "        # Find the same street and day of week in training data\n",
    "        historical = daily_street_train[\n",
    "            (daily_street_train['StreetName'] == street) &\n",
    "            (daily_street_train['DayOfWeek'] == dow)\n",
    "        ]\n",
    "        \n",
    "        if len(historical) > 0:\n",
    "            pred = historical['TicketCount'].mean()\n",
    "        else:\n",
    "            pred = daily_street_train['TicketCount'].mean()\n",
    "        \n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 6: TRAIN AND EVALUATE MODELS WITH TIME SERIES CROSS-VALIDATION\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 6] Training and evaluating models with time series cross-validation...\")\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Storage for results\n",
    "naive_rmse_scores = []\n",
    "naive_mape_scores = []\n",
    "seasonal_rmse_scores = []\n",
    "seasonal_mape_scores = []\n",
    "rf_rmse_scores = []\n",
    "rf_mape_scores = []\n",
    "poisson_rmse_scores = []\n",
    "poisson_mape_scores = []\n",
    "gb_rmse_scores = []\n",
    "gb_mape_scores = []\n",
    "\n",
    "fold = 1\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    daily_train = daily_street.iloc[train_idx]\n",
    "    daily_test = daily_street.iloc[test_idx]\n",
    "\n",
    "    # Naive baseline\n",
    "    naive_pred = naive_forecast(y_train, y_test)\n",
    "    naive_rmse = np.sqrt(mean_squared_error(y_test, naive_pred))\n",
    "    naive_mape = mean_absolute_percentage_error(y_test, naive_pred) * 100\n",
    "    naive_rmse_scores.append(naive_rmse)\n",
    "    naive_mape_scores.append(naive_mape)\n",
    "\n",
    "    # Seasonal naive baseline\n",
    "    seasonal_pred = seasonal_naive_forecast(daily_train, daily_test)\n",
    "    seasonal_rmse = np.sqrt(mean_squared_error(y_test, seasonal_pred))\n",
    "    seasonal_mape = mean_absolute_percentage_error(y_test, seasonal_pred) * 100\n",
    "    seasonal_rmse_scores.append(seasonal_rmse)\n",
    "    seasonal_mape_scores.append(seasonal_mape)\n",
    "\n",
    "    # Random Forest (baseline tree model)\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_pred = rf_model.predict(X_test)\n",
    "    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
    "    rf_mape = mean_absolute_percentage_error(y_test, rf_pred) * 100\n",
    "    rf_rmse_scores.append(rf_rmse)\n",
    "    rf_mape_scores.append(rf_mape)\n",
    "\n",
    "    # Enhanced Poisson Regression\n",
    "    poisson_model = PoissonRegressor(alpha=0.1, max_iter=1000)\n",
    "    poisson_model.fit(X_train, y_train)\n",
    "    poisson_pred = poisson_model.predict(X_test)\n",
    "    poisson_rmse = np.sqrt(mean_squared_error(y_test, poisson_pred))\n",
    "    poisson_mape = mean_absolute_percentage_error(y_test, poisson_pred) * 100\n",
    "    poisson_rmse_scores.append(poisson_rmse)\n",
    "    poisson_mape_scores.append(poisson_mape)\n",
    "\n",
    "    # Gradient Boosting\n",
    "    gb_model = GradientBoostingRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    gb_pred = gb_model.predict(X_test)\n",
    "    gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))\n",
    "    gb_mape = mean_absolute_percentage_error(y_test, gb_pred) * 100\n",
    "    gb_rmse_scores.append(gb_rmse)\n",
    "    gb_mape_scores.append(gb_mape)\n",
    "\n",
    "    print(f\"Naive RMSE: {naive_rmse:.3f}, MAPE: {naive_mape:.2f}%\")\n",
    "    print(f\"Seasonal Naive RMSE: {seasonal_rmse:.3f}, MAPE: {seasonal_mape:.2f}%\")\n",
    "    print(f\"Random Forest RMSE: {rf_rmse:.3f}, MAPE: {rf_mape:.2f}%\")\n",
    "    print(f\"Enhanced Poisson RMSE: {poisson_rmse:.3f}, MAPE: {poisson_mape:.2f}%\")\n",
    "    print(f\"Gradient Boosting RMSE: {gb_rmse:.3f}, MAPE: {gb_mape:.2f}%\")\n",
    "\n",
    "    fold += 1\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 7: SUMMARIZE RESULTS\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Naive Baseline', 'Seasonal Naive', 'Random Forest', 'Enhanced Poisson', 'Gradient Boosting'],\n",
    "    'Avg RMSE': [np.mean(naive_rmse_scores), np.mean(seasonal_rmse_scores), np.mean(rf_rmse_scores),\n",
    "                 np.mean(poisson_rmse_scores), np.mean(gb_rmse_scores)],\n",
    "    'Std RMSE': [np.std(naive_rmse_scores), np.std(seasonal_rmse_scores), np.std(rf_rmse_scores),\n",
    "                 np.std(poisson_rmse_scores), np.std(gb_rmse_scores)],\n",
    "    'Avg MAPE (%)': [np.mean(naive_mape_scores), np.mean(seasonal_mape_scores), np.mean(rf_mape_scores),\n",
    "                     np.mean(poisson_mape_scores), np.mean(gb_mape_scores)],\n",
    "    'Std MAPE (%)': [np.std(naive_mape_scores), np.std(seasonal_mape_scores), np.std(rf_mape_scores),\n",
    "                     np.std(poisson_mape_scores), np.std(gb_mape_scores)]\n",
    "})\n",
    "\n",
    "print(\"\\n\", results_df.to_string(index=False))\n",
    "\n",
    "# Calculate improvement over naive baseline\n",
    "baseline_rmse = results_df.loc[0, 'Avg RMSE']\n",
    "rf_improvement = ((baseline_rmse - results_df.loc[2, 'Avg RMSE']) / baseline_rmse) * 100\n",
    "poisson_improvement = ((baseline_rmse - results_df.loc[3, 'Avg RMSE']) / baseline_rmse) * 100\n",
    "gb_improvement = ((baseline_rmse - results_df.loc[4, 'Avg RMSE']) / baseline_rmse) * 100\n",
    "\n",
    "print(f\"\\n--- IMPROVEMENT OVER NAIVE BASELINE ---\")\n",
    "print(f\"Random Forest: {rf_improvement:.2f}% improvement in RMSE\")\n",
    "print(f\"Enhanced Poisson: {poisson_improvement:.2f}% improvement in RMSE\")\n",
    "print(f\"Gradient Boosting: {gb_improvement:.2f}% improvement in RMSE\")\n",
    "\n",
    "# Find the best performing model\n",
    "best_improvement = max(rf_improvement, poisson_improvement, gb_improvement)\n",
    "if best_improvement == poisson_improvement:\n",
    "    best_model = \"Enhanced Poisson\"\n",
    "elif best_improvement == rf_improvement:\n",
    "    best_model = \"Random Forest\"\n",
    "else:\n",
    "    best_model = \"Gradient Boosting\"\n",
    "\n",
    "if best_improvement >= 15:\n",
    "    print(f\"\\nâœ… SUCCESS: {best_model} achieved {best_improvement:.2f}% improvement, exceeding 15% target!\")\n",
    "else:\n",
    "    print(f\"\\nâš  Goal of 15% improvement not fully achieved. Best model: {best_model} with {best_improvement:.2f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 8: TRAIN FINAL MODELS ON FULL DATA\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 8] Training final models on full dataset...\")\n",
    "\n",
    "# Split into train/test (80/20)\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train_final = X.iloc[:split_idx]\n",
    "X_test_final = X.iloc[split_idx:]\n",
    "y_train_final = y.iloc[:split_idx]\n",
    "y_test_final = y.iloc[split_idx:]\n",
    "\n",
    "# Train final models\n",
    "final_rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_rf.fit(X_train_final, y_train_final)\n",
    "rf_final_pred = final_rf.predict(X_test_final)\n",
    "\n",
    "final_poisson = PoissonRegressor(alpha=0.1, max_iter=1000)\n",
    "final_poisson.fit(X_train_final, y_train_final)\n",
    "poisson_final_pred = final_poisson.predict(X_test_final)\n",
    "\n",
    "final_gb = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "final_gb.fit(X_train_final, y_train_final)\n",
    "gb_final_pred = final_gb.predict(X_test_final)\n",
    "\n",
    "print(\"Final models trained successfully.\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 9: GENERATE VISUALIZATIONS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 9] Generating visualizations...\")\n",
    "\n",
    "# 1. Model Comparison Bar Chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "models = results_df['Model']\n",
    "rmse_values = results_df['Avg RMSE']\n",
    "colors = ['#d62728', '#ff7f0e', '#2ca02c', '#1f77b4', '#9467bd']\n",
    "\n",
    "bars = plt.bar(models, rmse_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.axhline(y=baseline_rmse, color='red', linestyle='--', alpha=0.7, label='Naive Baseline')\n",
    "\n",
    "# Add improvement percentages\n",
    "improvements = [0, \n",
    "                ((baseline_rmse - results_df.loc[1, 'Avg RMSE']) / baseline_rmse) * 100,\n",
    "                rf_improvement, poisson_improvement, gb_improvement]\n",
    "\n",
    "for i, (bar, improvement) in enumerate(zip(bars, improvements)):\n",
    "    if improvement > 0:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                f'+{improvement:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.xlabel('Model', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Average RMSE', fontsize=12, fontweight='bold')\n",
    "plt.title('Model Performance Comparison (Cross-Validation)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../OUTPUT/model_comparison_rmse.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: model_comparison_rmse.png\")\n",
    "plt.close()\n",
    "\n",
    "# 2. Predictions vs Actual (Enhanced Poisson - Best Model)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_final, poisson_final_pred, alpha=0.6, s=25, color='green', edgecolor='black', linewidth=0.5)\n",
    "plt.plot([y_test_final.min(), y_test_final.max()],\n",
    "         [y_test_final.min(), y_test_final.max()],\n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Ticket Count', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Predicted Ticket Count', fontsize=12, fontweight='bold')\n",
    "plt.title('Enhanced Poisson Model: Predicted vs Actual Ticket Counts', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../OUTPUT/poisson_predictions_vs_actual.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: poisson_predictions_vs_actual.png\")\n",
    "plt.close()\n",
    "\n",
    "# 3. Feature Importance (Random Forest Model)\n",
    "feature_names = X.columns.tolist()\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': final_rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='steelblue', alpha=0.8)\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "plt.title('Feature Importance (Random Forest Model)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../OUTPUT/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: feature_importance.png\")\n",
    "plt.close()\n",
    "\n",
    "# 4. Residual Plot (Enhanced Poisson - Best Model)\n",
    "residuals = y_test_final - poisson_final_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(poisson_final_pred, residuals, alpha=0.6, s=25, color='green', edgecolor='black', linewidth=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "plt.xlabel('Predicted Ticket Count', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Residuals', fontsize=12, fontweight='bold')\n",
    "plt.title('Enhanced Poisson Model Residual Plot', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../OUTPUT/residual_plot.png', dpi=300, bbox_inches='tight')\n",
    "print(\"Saved: residual_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 10: SAVE RESULTS\n",
    "# =============================================================================\n",
    "print(\"\\n[STEP 10] Saving results...\")\n",
    "\n",
    "# Save model performance metrics\n",
    "results_df.to_csv('../OUTPUT/model_performance_metrics.csv', index=False)\n",
    "print(\"Saved: model_performance_metrics.csv\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv('../OUTPUT/feature_importance.csv', index=False)\n",
    "print(\"Saved: feature_importance.csv\")\n",
    "\n",
    "# Save final predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test_final.values,\n",
    "    'Random_Forest_Predicted': rf_final_pred,\n",
    "    'Poisson_Predicted': poisson_final_pred,\n",
    "    'GradientBoosting_Predicted': gb_final_pred,\n",
    "    'RF_Error': y_test_final.values - rf_final_pred,\n",
    "    'Poisson_Error': y_test_final.values - poisson_final_pred,\n",
    "    'GB_Error': y_test_final.values - gb_final_pred\n",
    "})\n",
    "predictions_df.to_csv('../OUTPUT/final_predictions.csv', index=False)\n",
    "print(\"Saved: final_predictions.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PARKING TICKET PREDICTION ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAll outputs saved to ../OUTPUT/ directory\")\n",
    "print(f\"Best performing model: {best_model} with {best_improvement:.1f}% improvement over baseline\")\n",
    "print(f\"\\nKey insights:\")\n",
    "print(f\"1. Enhanced Poisson regression performs best for count data prediction\")\n",
    "print(f\"2. Tree-based models (Random Forest, Gradient Boosting) provide competitive performance\")\n",
    "print(f\"3. Enhanced features significantly improve prediction accuracy across all models\")\n",
    "print(f\"4. Temporal and street-level features are crucial for parking ticket prediction\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Deploy best performing model for operational parking enforcement\")\n",
    "print(\"2. Integrate with external data sources (events, weather)\")\n",
    "print(\"3. Implement real-time prediction system\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
